\documentclass[11pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[spanish,es-lcroman]{babel} 
\usepackage{graphicx}
\usepackage{mdframed}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{amsmath, amssymb, amsfonts}
\usepackage{amsthm}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{textcomp}
\usepackage{placeins}
\usepackage{etoolbox}
\usepackage{tikz}

\usetikzlibrary{decorations.pathreplacing}

\spanishdecimal{.}
\graphicspath{{images/}}
%COMILLAS “”
\title{Resumen Teórico Matemática Discreta II}
\author{Ferré Valderrama, Eduardo}
\date{\today}

\theoremstyle{definition}
\newtheorem{definition}{Definición}
\newtheorem{theorem}{Teorema}
\newtheorem{lemma}{Lema}
\newtheorem*{demostracion}{Demostración}
\newtheorem{corollary}{Corolario}
\newtheorem{proposition}{Proposición}
\newtheorem{example}{Ejemplo}
\newtheorem{exercise}{Ejercicio}
\newtheorem*{remark}{Observación}
\newtheorem{notation}{Notación}

%\qed(cuadradito vacio de que termino la demostración%
%\blacksquare(cuadradito lleno de que termino la demostración%

\begin{document}
\maketitle
Matemática Discreta II
\tableofcontents

\section{¿Cuál es la complejidad de Edmons-Karp?}

La complejidad del algoritmo de Edmons-Karp es $O(nm^2)$\\
\textbf{Demostración}
    \begin{itemize}
        \item E-K construye una sucesión de caminos aumentantes, cada uno de ellos se encuentra utilizando BFS, que es $O(m)$.
        \item Por lo tanto la complejidad de E-K es $O(m)$ por $\#$caminos aumentantes.
        \item Entonces si demostramos que en E-K, $\#$caminos aumentantes  $= O(nm)$, tenemos el teorema.
        \item Para probar esto tenemos que ver cuantas veces un lado puede volverse crítico, teniendo en cuenta que un lado se vuelve crítico en el paso $k$ cuando al pasar al paso $k+1$ este se satura o se vacía.
        \item Esto es porque por cada camino aumentante que encontremos, cuando aumentemos el flujo, vamos a saturar o llenar al menos un lado en ese camino.
        \item Entonces si acotamos la cantidad de veces que un lado se vuelve crítico, habremos acotado la cantidad de caminos aumentantes
    \end{itemize}
    Supongamos que un lado $\overrightarrow{xy}$ se vuelve crítico en el paso $k$, y luego en el paso $j$, con $k<j$. Entonces hay dos casos para analizar
    \begin{itemize}
        \item \textbf{Caso 1.} Se vuelve crítico en el paso $k$ porque se saturó.
        \item \textbf{Caso 2.} Se vuelve crítico en el paso $k$ porque se vació.
    \end{itemize}
    
    \begin{itemize}
        \item \textbf{Caso 1. En el que se satura (forward)}: Como se saturó en el paso $k$ entonces:
            \begin{itemize}
                \item Para construir $f_{k+1}$ se debe usar un $f_k$ camino aumentante de la forma $s \ldots \overrightarrow{xy} \ldots t$.
                \item Como estamos usando EK ese camino es de longitud mínima, por lo tanto: 
            \end{itemize}
                \[
                    d_k(y)=d_k(x)+1 \tag{1}
                \]
            Para que se vuelva crítico en el paso $j$, debe pasar una de estas dos cosas:
            \begin{itemize}
                \item Se vació, es decir usamos $\overrightarrow{xy}$ backward en el paso $j$.
                \item Se saturó, como luego del paso $k$ ya se saturó, para saturarse de nuevo debemos haberle devuelto el flujo en un paso $i$, con $k<i<j$.
            \end{itemize}
            Entonces en cualquiera de estos dos casos, deducimos que existe $i$ tal que $k < i < j$ tal $\overrightarrow{xy}$ se usa en backward.\\ \\ 
            Esto implica que para construir $f_{i+1}$ se usa un $f_i-$camino aumentante de la forma $s \ldots \overleftarrow{yx} \ldots t$.
            Como estamos usando EK, ese camino es de longitud mínima, por lo tanto:
                \[
                    d_i(x)=d_i(y)+1 \tag{2}
                \]
            Entonces:
            \begin{align*}
                d_j(t) \geq d_i(t) &= d_i(x) + b_i(x). \\[6pt]
                &= d_i(y) + 1 + b_i(x) \quad \text{Por \textbf{(2)}} \\[6pt]
                &\geq d_k(y) + 1 + b_k(x) \quad \text{Porque las distancias no disminuyen} \\[6pt]
                &= d_k(x) + 1 + 1 + b_k(x) \quad \text{Por \textbf{(1)}} \\[6pt]
                &= d_k(t) + 2
                \end{align*}
                
                Este fue el análisis de si el lado $\overrightarrow{xy}$ se volvía crítico en el paso $k$ porque se saturaba. Ahora vamos con el 
                \textbf{Caso 2.}
            
        \item \textbf{Caso 2. En el que se vacía (backward)}: Como se vacía, existe un camino (de longitud mínima) de la forma 
                $s \ldots \overleftarrow{yx} \ldots t$ que se usa para pasar de $f_k$ a $f_{k+1}$. Por lo tanto:
                \[
                    d_k(x)=d_k(y)+1 \tag{3}
                \]
                Para que se vuelva crítico en el paso $j$, debe o bien volverse a vaciar, o bien saturarse, en el segundo de esos casos estamos 
                obviamente mandando más flujo en el paso $j$, pero en el primero si estamos diciendo que debe vaciarse de vuelta, entonces antes 
                debe haberse llenado aunque sea un poco. \\ \\ 
                Así que en cualquier caso debe haber un $i$ tal que $k<i<j$ para el cual se manda flujo a través de él. Entonces, existe un camino 
                (de longitud mínima) de la forma $s \ldots \overrightarrow{xy} \ldots t$ que se usa para pasar de $f_i$ a $f_{i+1}$. Por lo tanto:
                \[
                    d_l(y)=d_l(x)+1 \tag{4}
                \]
                Entonces:
                \begin{align*}
                    d_j(t) \geq d_i(t) &= d_i(y) + b_i(y). \\[6pt]
                    &= d_i(x) + 1 + b_i(y) \quad \text{Por \textbf{(4)}} \\[6pt]
                    &\geq d_k(x) + 1 + b_k(y) \quad \text{Porque las distancias no disminuyen} \\[6pt]
                    &= d_k(y) + 1 + 1 + b_k(x) \quad \text{Por \textbf{(3)}} \\[6pt]
                    &= d_k(t) + 2
                    \end{align*}
                Entonces tanto como para el \textbf{Caso 1} como para el \textbf{Caso 2}, $d_j(t) \geq d_k(t)+2$.\\
                Es decir, una vez un lado se vuelve lado crítico, para que se vuelva 
                crítico otra vez, la distancia entre $s$ y $t$ debe aumentar al menos en 2.\\ \\
                Como la distancia entre $s$ y $t$ puede ir desde mínimo de $1$ hasta un máximo de $n-1$, entonces un lado puede volverse crítico 
                un máximo de \textbf{$O(\frac{n}{2}) = O(n)$ veces}.\\ \\
                Entonces finalmente:
                \begin{itemize}
                    \item Como cada camino aumentante que se usa en EK tiene al menos un lado que se vuelve crítico
                    \item Cada lado se puede volver crítico a lo sumo $O(n)$ veces
                    \item Hay $m$ lados
                    \item $\Rightarrow$ hay $O(nm)$ caminos aumentantes.
                \end{itemize}
                Finalmente concluimos que la complejidad total de EK es 
                \[
                O(m)*O(nm) = O(nm^2) \blacksquare
                \]   
    \end{itemize}
\section{Probar que, si dados vértices $x,z$ y flujo $f$ definimos la distancia entre $x$ y $z$ relativa a $f$ como la longitud del menor
        $f$-camino aumentante entre $x$ y $z$, si es que existe tal camino, o infinito si no existe o 0 si $x=z$, denotándola por $d_f(x,z)$,
        y definimos $d_k(x)=d_{f_k}(s,x)$, donde $f_k$ es el $k$-ésimo flujo en una corrida de Edmonds-Karp, entonces $d_k(x)$ $\leq$ $d_{k+1}(x)$.}

\begin{demostracion}
    Vamos a demostrar que $d_k(x) \leq d_{k+1}(x)$.\\ \\
    Sea $A=\{y: d_{k+1}(y)<d_k(y)\}$. Queremos ver que $A=\emptyset$, supongamos que no.\\ \\
    Sea $x \in A$ tal que
    \begin{equation}
        d_{k+1}(x)=min\{d_{k+1}(y): y \in A\}
    \end{equation}
    Observemos que $x \in A$, entonces $d_{k+1}(x)<d_k(x) \leq \infty  \Rightarrow d_{k+1} < \infty$. Entonces existe un $f_{k+1}$-camino aumentante
    entre $s$ y $x$. 
    \begin{remark}
        $x \neq s$, pues $s\notin A$, pues $d_k(s)=d_{k+1}(s)=0$.
    \end{remark}
    Sea entonces $p_{k+1}:=s \ldots x$ un $f_{k+1}$-camino aumentante de longitud mínima entre $s$ y $x$, es decir, de longitud $d_{k+1}(x)$. Como 
    $x\neq s$ en ese camino existe un vertice inmediatamente anterior a $x$, llamemosle $z$. Entonces, $p_{k+1}=s \ldots zx$.\\
    Como $p_{k+1}$ es de longitud mínima entre $s$ y $x$, entonces es de longitud mínima entre $s$ y cualquier otro vertice intermedio como pej $z$. Por 
    lo tanto:
    \begin{equation}
        d_{k+1}(z)=d_{k+1}(x)-1
    \end{equation}
    Como $p_{k+1} = s \ldots zx$ es un $f_{k+1}$-camino aumentante, entonces existe el lado $\overrightarrow{zx}$ o existe el lado 
    $\overrightarrow{xz}$. Analicemos los dos casos.
    \begin{itemize}
        \item \textbf{Caso 1.} Si $\overrightarrow{zx}$ es un lado, tenemos que $d_{k+1}(z) < d_{k+1}(x)$. Pero
         
        \[ z \notin A \implies d_k(z) \leq d_{k+1}(z) < d_{k+1}(x) < \infty \]
        
        Como $d_k(z) < \infty$, existe un $f_k$-camino aumentante entre $s$ y $z$. 
        Lo denominamos $p_k$. Como $\overrightarrow{zx}$ es un lado, podría, en principio, agregar $x$ al final de $p_k$, 
        obteniendo un $f_k$-c.a. entre $s$ y $x$ que pasa por $z$. Pero entonces tendríamos 
        $d_k(x) \leq d_k(z) + 1 \leq d_{k+1}(z) + 1 \leq d_{k+1}(x)$. 
        Pero esto implica $x \notin A$. 
        
        Por lo tanto, no puedo agregar $x$ al final de $p_k$. 
        Entonces, o bien el lado $\overrightarrow{zx}$ está saturado en el paso $k$, o bien $f_k(\overrightarrow{zx}) = c(\overrightarrow{zx})$. 
        Pero $p_{k+1}$ era aumentante; es decir, sus lados no pueden estar saturados; 
        lo cual implica que $f_{k+1}(\overrightarrow{zx}) < c(\overrightarrow{zx})$ en el paso $k + 1$. 
        Luego, para pasar de $f_k$ a $f_{k+1}$, se usó el lado $\overrightarrow{zx}$ y se usó backwards en el camino
        
        \[ 
        \tilde{p}_k : s \ldots \overleftarrow{xz} \ldots t 
        \]
        
        Como este camino se usa en Edmonds-Karp, es de longitud mínima. Por lo tanto,
        
        \[ d_k(z) = d_k(x) + 1 \]Luego,
        \begin{align*}
            d_k(z) & = d_k(x) + 1 \\
                   & > d_{k+1}(x) + 1 \\
                   & = d_{k+1}(z) + 2 \\
                   & \geq d_k(z) + 2
        \end{align*}
        Absurdo pues, $0 < 2$.
        


        \item \textbf{Caso 2.} Si $\overrightarrow{xz}$ es un lado, tenemos que $p_{k+1}$ es de la forma $s \ldots \overrightarrow{zx}$. Como antes, 
        \[ d_{k+1}(z) < d_{k+1}(x) \implies z \in A \implies d_k(z) \leq d_{k+1}(z) \]
        lo cual a su vez implica que existe un $f_k$-camino aumentante entre $s$ y $z$:
        \begin{center}
            $p_k: s \ldots z$
        \end{center}
        Podríamos agregar a $x$ en el final de este camino usando $\overrightarrow{xz}$ backward: llegaríamos a la misma contradicción que antes, en 
        que no podríamos agregar $x$. En este caso, si no lo podemos agregar, es porque $f_k(\overrightarrow{xz})=0$. Pero $p_{k+1}$ es 
        $f_{k+1}$-aumentante, lo cual implica que $f_{k+1}(\overrightarrow{xz})>0$. Luego debimos haber usado $\overrightarrow{xz}$ en modo forward al 
        pasar de $f_k$ a $f_{k+1}$. Por lo tanto existe un $f$-camino aumentante de la forma:
        \begin{center}
            $\tilde{p}_k: s \ldots \overrightarrow{xz} \ldots t$
        \end{center}
        Y luego como usamos Edmonds-Karp, este camino aumentante es de longitud mínima.\\
        Luego $d_k(z) = d_k(x) + 1$.\\
        \begin{align*}
            d_k(z) & = d_k(x) + 1 \\
                   & > d_{k+1}(x) + 1 \\
                   & = d_{k+1}(z) + 2 \\
                   & \geq d_k(z) + 2
        \end{align*}
        Absurdo pues, $0 < 2$. 
    \end{itemize}
    Finalmente concluimos que en ambos casos llegamos a un absurdo, y este absurdo viene de suponer que \({A \ne \emptyset}\), entonces A es vacío y finalmente 
    \[ d_k(x) \leq d_{k+1}(x) \]
\end{demostracion}

\section{Probar que, si dados vértices $x,z$ y flujo $f$ definimos la distancia entre $x$ y $z$ relativa a $f$ como la longitud del menor
        $f$-camino aumentante entre $x$ y $z$, si es que existe tal camino, o infinito si no existe o 0 si $x=z$, denotándola por $d_f(x,z)$,
        y definimos $b_k(x)=b_{f_k}(x,t)$, donde $f_k$ es el $k$-ésimo flujo en una corrida de Edmonds-Karp, entonces $b_k(x)$ $\leq$ $b_{k+1}(x)$.}

\begin{demostracion}
    \textbf{Se evalúa en diciembre de 2024.}
\end{demostracion}

\section{¿Cual es la complejidad del algoritmo de Dinic? Probarla en ambas versiones: Dinitz original y Dinic-Even. 
(no hace falta probar que la distancia en networks auxiliares sucesivos aumenta).}
La complejidad de Dinitz, en su versión original como occidental, es $O(n^2m)$.

\begin{demostracion}
    Como el nivel de $t$ en los networks auxiliares aumenta, solo puede haber $O(n)$ networks auxiliares. 
    \(\therefore\)La complejidad de Dinitz es igual a la complejidad de hallar un flujo bloqueante en un network auxiliar, 
    más la complejidad de construir un network auxiliar, 
    por la cantidad de networks auxiliares---que es $O(n)$. 
    Porque usamos DFS, la complejidad de construir un network auxiliar es $O(m)$. 
    Entonces basta probar que la complejidad de hallar un flujo bloqueante es $O(nm)$, porque tendremos
    \[ O(n) \cdot (O(m) + O(nm)) = O(n^2m) \]

    \newpage

    \begin{itemize}
        \item \textbf{Versión Original: } \\ \\
        En la versión original del algoritmo, el n.a. es tal que no hay vértices sin lado de salida. Esto implica que DFS siempre llega a $t$ sin hacer backtracking. Entonces, DFS no es $O(m)$ sino $O$(cantidad de niveles) que podemos simplificar como $O(n)$. Como cada camino borra al menos un lado del n.a. hay $O(m)$ caminos. Luego la complejidad de hallar los caminos y aumentar el flujo por ellos es $O(nm)$. Pero debemos calcular el costo de que el n.a. satisfaga la propiedad de que no hay vértices sin lados de salida.

        Para esto, luego de cada camino hay que hacer algo para mantener esa propiedad como invariante. Dinitz llamó a la operación que logra esto \textbf{``podar''}. La operación consiste en recorrer los vértices desde los niveles más altos a los más bajos, chequea si el vértice tiene lados de salida; si tiene no hace nada, si no lo tiene lo borra.
        
        Chequear si el lado tiene lados de salida es $O(1)$; hay $O(n)$ vértices, y hay un ``podar'' por cada camino y hay $O(m)$ caminos. Esto entonces también es $O(nm)$.
        
        Nos queda analizar la complejidad de borrar el vértice y sus lados; esto se hace a lo sumo una vez por vértice, y una vez que podamos un vértice es probable que podar los siguientes requiera una complejidad menor (porque estamos quitando lados). Esto nos dice que podemos pensar en la complejidad promedio del problema, y no en la complejidad límite (es decir como cota).
        
        Borrar un vértice $x$ y sus lados es $O(d(x))$; sobre todos los vértices, esto da $\sum O(d(x)) = O(m)$ (por la propiedad del apretón de manos). Luego la complejidad de hallar un flujo bloqueante es:
        
        \[
        O(nm) + O(nm) + O(m) = O(nm) \blacksquare
        \]
        \item \textbf{Versión Dinic-Even: } \\
        Ahora consideremos la versión occidental. Lo más cómodo es dar el pseudocódigo. Estamos hablando del network auxiliar y el problema de hallar un flujo bloqueante en él.

        Para hallar flujo bloqueante $g$ en un n.a:
        \begin{algorithm}[!ht]
            \caption{Algoritmo}
            \begin{algorithmic}[1]
            \State $g := 0$
            \State $bool\ flag := true$
            \While{$flag$}
               \State type $path := [s]$
               \State $int\ x := s$
               \While{$x \neq t$}
                   \If{$\Gamma^+(x) \neq 0$}
                       \State tomar $y \in \Gamma^+(x)$
                       \State agregar $y$ a $path$
                       \State $x := y$ \Comment{Esta línea y la anterior son la parte (A) de avanzar}
                   \Else
                       \If{$x \neq s$}
                           \State $z :=$ elemento anterior a $x$ en $path$ \Comment{(R) Retroceder}
                           \State borro $x$ de $path$
                           \State borro $\overrightarrow{zx}$ de la n.a.
                           \State $x := z$
                       \Else
                           \State $flag := 0$
                       \EndIf
                   \EndIf
               \EndWhile
               \If{$x = t$}
                   \State aumentar flujo $g$ a lo largo de $path$ \Comment{(I) Incrementar}
                   \State borrar lados saturados
               \EndIf
            \EndWhile
            \State \textbf{return} $g$
            \end{algorithmic}
            \end{algorithm}
            \FloatBarrier

            Entonces, una corrida de Dinitz occidental es una palabra $AAAIAAARARRAA\ldots$
            Miraremos subpalabras de la forma $A\ldots AX$ con $X \in \{I, R\}$. 
            \begin{itemize}[noitemsep, nolistsep]
                \item Cada $A$ es $O(1)$;
                \item Cada $R$ es $O(1)$;
                \item Cada $I$ es recorrer un camino dos veces (una para incrementar el flujo, otra para borrar los lados), y resulta ser $O(n)$.
            \end{itemize}
            La cantidad de A's en $A\ldots AX$ se calcula como sigue. Cada avanzar, mueve el pivote
            $x$ de un nivel al siguiente. $\therefore$ Hay $O(n)$ letras A.
            \begin{itemize}[noitemsep, nolistsep]
                \item Cada avanzar, mueve el pivote $x$ de un nivel al siguiente. $\therefore$ Hay $O(n)$ letras A.
                \item $\therefore O(A\ldots AR) = O(n) + O(1) + O(n)$
                \item y $O(A\ldots AI) = O(n)(\#A) + O(n)(\#I) = O(n)$
            \end{itemize}
            Donde $\#Z$ es la cantidad de veces que ocurre la letra $Z$ en la palabra en cuestión. En
            resumen, $O(A\ldots AX) = O(n)$. Y la cantidad de palabras $A\ldots AX$ en una corrida se
            calcula así: 
            \begin{itemize}[noitemsep, nolistsep]
                \item Cada $R$ borra un lado, 
                \item y cada $I$ borra \textit{al menos} un lado;
                \item luego hay $O(m)$ palabras $A\ldots AX$.
            \end{itemize}
              Luego la complejidad total es $O(nm)$. $\blacksquare$
    \end{itemize}
\end{demostracion}


\section{¿Cual es la complejidad del algoritmo de Wave? Probarla.}
La complejidad de Wave es $O(n^3)$.

\begin{demostracion}
    Como es un algoritmo de tipo Dinitz, la distancia en n.a aumenta. Por lo tanto hay $O(n)$ networks auxiliares. Basta probar que la complejidad de 
    hallar un flujo bloqueante en Wave es $O(n^2)$.\\
    Dividiremos la complejidad en varias partes. 

    \begin{itemize}
        \item Los \textbf{“Los balanceos hacia adelante”} de los vertices.\\
                Cuando un vertice $x$ manda flujo a $z$, puede pasar que $\overrightarrow{xz}$ se sature o no. Sea entonces:
                \begin{itemize}
                    \item $S=$ Todos los pasos donde se satura un lado.
                    \item $P=$ Todos los pasos donde no se satura un lado.
                \end{itemize}
        \item Los \textbf{“Los balanceos hacia atrás”} el razonamiento es análogo.\\
                Cuando devolvemos flujo, podemos devolverlo todo o solo un poco. Sea entonces:
                \begin{itemize}
                    \item $V=$ La parte donde se vacían los lados.
                    \item $Q=$ La parte donde se devuelve flujo parcialmente.
                \end{itemize}
    \end{itemize}
    Entonces la complejidad de hallar un flujo bloqueante en Wave es 
    \begin{center}
        $S+P+V+Q$
    \end{center} 
    Analicemos cada una en detalle.

    \begin{itemize}
        \item \textbf{Análisis de $S$}\\
        Como habíamos mencionado, cuando un vertice $x$ manda flujo a $z$, puede pasar que $\overrightarrow{xz}$ se sature o no. Supongamos que 
        $\overrightarrow{xz}$ se saturó. La pregunta es si puede volver a saturarse en alguna ola hacia adelante. Para volver a saturarse, debe primero
        des-saturarse.
        \begin{center}
            Por lo tanto, $z$ debe devolverle flujo a $x$ $\Rightarrow$ $z$ debe estar “bloqueado”.
        \end{center} 
        Pero, en las olas hacia adelante, cuando un vértice chequea sus vecinos, solo se manda flujo a vertices no bloqueados. Entonces si $z$ le 
        devuelve flujo a $x$, $x$ no le puede volver a mandar flujo a $z$, por lo tanto:
        \begin{itemize}
            \item $\overrightarrow{xz}$ no se puede volver a saturar otra vez.
            \item Entonces cada lado se satura a lo sumo una vez.
        \end{itemize}
        Entonces concluimos que la complejidad de $S$ es $O(m)$.
        \item \textbf{Análisis de $V$}\\
        La pregunta es cuantas veces puede vaciarse un lado. La respuesta es muy similar al caso de $S$. 
        Si $x$ devuelve flujo a un vertice $u$ y se vacía $\overrightarrow{ux}$, entonces $x$ debe estar bloqueado, por lo tanto:
        \begin{itemize}
            \item $u$ nunca puede volver a enviar flujo.
            \item Por lo tanto, $\overrightarrow{ux}$ no puede volver a vaciarse (porque no se puede volver a enviar nada).
            \item Entonces cada lado se vacía a lo sumo una vez.
        \end{itemize}
        Entonces concluimos que la complejidad de $V$ es $O(m)$.
        \item \textbf{Análisis de $P$ y $Q$}\\
        Cuando intentamos balancear hacia adelante desde un vértice,
        todos los lados excepto quizas el ultimo se saturan. Es decir, en cada vértice de
        cada ola hacia adelante, hay a lo sumo un lado que no se satura, y “cuenta” para la
        complejidad de $P$. 
        \begin{itemize}
            \item Enviar flujo por ese lado es $O(1)$. 
            \item Luego la complejidad de $P$ es $O(n)$$*$$(\#$olas hacia adelante$)$.
        \end{itemize}
        Similar en $Q$.

        Al balancear hacia atras, en cada vértice se vacían todos los lados excepto a lo sumo uno. 
        \begin{itemize}
            \item Luego la complejidad de $Q$ es $O(n)$$*$$(\#$olas hacia atrás$)$.
        \end{itemize}
        Luego tenemos que:
        \begin{center}
            $\#$olas hacia adelante $=$ $\#$olas hacia atrás
        \end{center}
        Pues a cada ola hacia adelante corresponde una ola hacia atras. Ahora bien tenemos que calcular la cantidad de olas.

        En cada ola hacia adelante excepto la ultima (cuando se logra el balanceo), algun vertice queda desbalanceado. 
        Pero entonces la ola debio de bloquearlo. Por lo tanto, en cada ola hacia adelante excepto tal vez la ultima, 
        se bloquea al menos un vertice. Los vertices nunca se desbloquean.
        \begin{itemize}
            \item Luego hay $O(n)$ olas.
            \item Entonces la complejidad de $P$ es igual a la de $Q$
            \item Esto es $O(n)*O(n) = O(n^2)$
        \end{itemize}
        Finalmente tenemos que la complejidad de hallar un flujo bloqueante en Wave es:
        \begin{align*}
            S+P+V+Q &= O(m) + O(n^2) + O(m) + O(n^2) \\[6pt]
            &= O(n^2) + O(n^2) + O(m) + O(m) \\[6pt]
            &= O(n^2) + O(m) \quad \text{y} \quad m=O(n^2) \\[6pt]
            &= O(n^2) \blacksquare \\[6pt]
        \end{align*}    
    \end{itemize}
\end{demostracion}

\section{Probar que la distancia en networks auxiliares sucesivos aumenta.}
\textbf{Se evalúa en diciembre de 2024.}

\section{Probar que el valor de todo flujo es menor o igual que la capacidad de todo corte y que si $f$ es un flujo, entonces $f$ es
maximal si y solo si existe un corte $S$ tal que $v(f )=cap(S)$. (y en este caso, S es minimal)
(puede usar sin necesidad de probarlo que si $f$ es flujo y $S$ es corte entonces $v(f )=f(S,\overline{S})-f(\overline{S}, S)$)}


\textbf{Demostración.}
    \begin{enumerate}
        \item Probaremos que $f\leq Cap(S)$ para un corte $S$ y un flujo $f$ arbitrarios. Por lema sabemos que:
        \begin{center}
            $v(f)=f(S,\overline{S})-f(\overline{S},S)$
        \end{center}
        Observemos que el término $f(\overline{S},S)$ es de la forma $\sum f(\overrightarrow{xy})$. Por definición, cada término en esa sumatoria
        es mayor o igual a cero, y por lo tanto la sumatoria es positiva. Entonces tenemos que $-f(\overline{S},S) \leq 0$. Luego
        \begin{center}
            $v(f)=f(S,\overline{S})-f(\overline{S},S) \leq f(S,\overline{S})$
        \end{center}
        El primer término:$f(S,\overline{S})$, también es una sumatoria de términos positivos y es por lo tanto mayor a cero, pero cada término es 
        a su vez menor a la capacidad de cada lado, es decir $f(S,\overline{S})\leq c(S,\overline{S})=Cap(S)$. Por lo tanto:
        \begin{center}
            $v(f)\leq f(S,\overline{S})\leq Cap(S)$ $\blacksquare$
        \end{center}
        \item Ahora probemos que $f$ es maximal $\Longleftrightarrow$ existe un corte minimal $S$ tal que $v(f)=Cap(S)$.
        \begin{itemize}
            \item \textbf{($\Leftarrow$)} Sea ahora $f$ un flujo y $S$ un corte con $v(f)=Cap(S)$. Sea $g$ cualquier otro flujo. Entonces, por la 
                propiedad recien demostrada, $v(g)\leq Cap(S)$. Pues $Cap(S)=v(f)$, tenemos $v(g)\leq v(f)$. Por lo tanto $f$ es maximal. Como 
                detalle si $T$ es un corte, $Cap(T)\geq v(f)=Cap(S)$, lo cual implica que $S$ es minimal.
            \item \textbf{($\Rightarrow$)} Asumimos que \( f \) es maximal. Probaremos que existe un corte \( S \) con \( v(f) = C a p(S) \). 
            Para esto, debemos construir \( S \) a partir \( f \). Definiremos

            \[ S = \{s\} \cup \{ x \in V : \exists f\text{-camino aumentante entre } s \text{ y } x \} \]
            
            Que \( S \) es un corte se sigue por contradicción. Si \( S \) no es corte, debe contener a \( t \). 
            Luego existe un \( f\text{-camino aumentante desde } s \text{ a } t \). 
            Esto implica que puedo aumentar el flujo \( f \), lo cual contradice que \( f \) es maximal. 
            
            Sabiendo que \( S \) es un corte, tenemos
            
            \[ v(f) = f(S, \overline{S}) - f(\overline{S}, S) \tag{1} \]
            
            Consideremos el primer término en la resta de (1):
            
            \[ f(S, \overline{S}) = \sum_{\substack{x \in S \\ z \notin S \\ \overrightarrow{xz} \in E}} f(\overrightarrow{xz}) \]
            
            Sea \(\overrightarrow{xz}\) un par dentro del rango de la suma de arriba. 
            Pues \( x \in S \), existe un \( f\text{-camino aumentante de } s \text{ a } x \). 
            Pues \( z \notin S \), no existe un \( f\text{-camino aumentante de } s \text{ a } z \). 
            Pero el lado \(\overrightarrow{xz}\) sí existe. Así que \( s \ldots xz \) podría ser un \( f\text{-camino aumentante} \); 
            como tal camino no existe por hipótesis, no es aumentante. 
            Esto implica que \( f(\overrightarrow{xz}) = c(\overrightarrow{xz}) \). 
            La conclusión es que \( f(\overrightarrow{xz}) = c(\overrightarrow{xz}) \) para todo \( x \in S, z \notin S, \overrightarrow{xz} \in E \). 
            Entonces

            \[ f(S, \overline{S}) = \sum_{\substack{x \in S \\ z \notin S \\ \overrightarrow{xz} \in E}} f(\overrightarrow{xz}) = \sum_{\substack{x \in S \\ z \notin S \\ \overrightarrow{xz} \in E}} c(\overrightarrow{xz}) = Cap(S) \]

            Ahora consideremos el segundo término de la ecuación (1).

            \[ f(\overline{S}, S) = \sum_{\substack{w \notin S \\ x \in S \\ \overrightarrow{wx} \in E}} f(\overrightarrow{wx}) \]

            Sea \(\overrightarrow{wx}\) un par arbitrario en el rango de la suma. Como antes, \( x \in S \Rightarrow \) que hay un camino aumentante de \( s \) a \( x \); pero no existe camino aumentante entre \( s \) y \( w \); pero \(\overrightarrow{wx}\) es un lado. Es decir, \( s \ldots \overleftarrow{xw} \) podría ser un \( f\text{-camino aumentante}\); y como no lo es sucede que \( f(\overrightarrow{wx}) = 0 \). Luego

            \[ f(\overline{S}, S) = \sum_{\substack{w \notin S \\ x \in S \\ \overrightarrow{wx} \in E}} f(\overrightarrow{wx}) = 0 \]

            Luego \( v(f) = Cap(S) - 0 = Cap(S) \). \(\blacksquare\)
        \end{itemize}

    \end{enumerate}


\section{Probar que si $G$ es conexo no regular, entonces $\chi(G)\leq \Delta(G)$.}

Sea $x$ tal que $d(x)=\delta(G)$. Corramos BFS empezando por $x$ y guardamos el orden inverso en el que se visitaron los vértices.
Ahora corramos Greedy en el orden que acabamos de guardar. Notemos que en BFS todo vertice es incluido por un vertice que ya ha sido visitado, entonces en el orden 
inverso todo vertice tiene al menos un vecino por delante, excepto $x$. Entonces para cada vertice $y$, en el peor caso va a tener $d(y)-1 < \Delta$ vecinos coloreados,
todos con un color distinto. Por lo tanto podemos elegir un color entre $1 \ldots \Delta$.\\ \\ 
Finalmente, cuando llegamos a $x$, como $d(x)=\delta$, podemos elegir algún color 
entre $1 \ldots \Delta$ que no haya sido usado por sus vecinos para colorearlo. Por lo tanto encontramos un coloreo propio para $G$ que usa (a lo sumo) $\Delta$ colores.\\ \\
Por lo tanto:
\[
\chi(G) \leq \Delta(G) \blacksquare
\]

\section{Probar que 2-COLOR es polinomial}

\begin{demostracion}

Para esta demostración vamos a dar un algoritmo polinomial que colorea un grafo con dos
colores.\\ 
Luego probaremos que si el coloreo que da nuestro algoritmo no es propio
es porque en el grafo hay algún ciclo impar, lo que implicaría $\chi(G) \geq 3$.\\
Notar que vamos a asumir que el grafo es conexo, sin embargo si no lo es,
simplemente hay que correr el algoritmo en todas sus componentes conexas
y ver que todas sean bipartitas.\\ \\
\textbf{Algoritmo}\\ \\
    Tomamos un $x \in V$, corremos BFS empezando en $x$. Si:
    \begin{align*}
        N(z) &= \text{nivel } z \text{ en el árbol BFS} \\[6pt]
             &= \text{distancia entre } z \text{ y } x \quad  \text{ en el árbol BFS} \\[6pt]
             &= \text{distancia entre } z \text{ y } x \quad  \text{ en } G \\[6pt]
    \end{align*}
    Sea:
    \begin{itemize}
        \item $C(z) = (N(z) \mod 2)$
        \item \textbf{If} ($C$ es propio, return si, es 2-colorable).
        \item \textbf{Else} (return no, no es 2-colorable).
    \end{itemize}
    El algoritmo es polinomial porque BFS es $O(m)$ y chequear que es propio es $O(m)$, ahora veamos que pasa si nuestro coloreo no es propio .\\
    Entonces tenemos que:     
    \[
    \exists v, z : c(v) = c(z) \land vz \in E
    \]
    Entonces $d(x, v) = d(x, z) \mod 2$\\
    Tomamos un camino entre $x$ y $v$ en BFS y un camino entre $x$ y $z$ en BFS y sea $w$ el único vértice en común, como en la siguiente figura:
    \begin{figure}[!h]
        \centering 
        \includegraphics[height=4cm]{Figure 1 2-color.png} 
    \end{figure}
    
    Miramos el ciclo en $G$: $w \ldots \underset{\text{cruzo a }z}{\underbrace{vz}} \ldots \underset{\text{vuelvo a }x}{\underbrace{w}}$.\\ \\
    Calculamos la longitud de este ciclo:
    \begin{align*}
    \text{longitud} &= 1 + d(v, w) + d(z, w) \\
    \text{longitud mod 2} &= (1 + d(v, w) + \ldots + d(z, w)) \text{ mod } 2 \\
    &= (1 + d(v, w) + d(z, w) + 2d(x, w)) \text{ mod } 2 \\
    &= (1 + d(x, v) + d(x, z)) \text{ mod } 2 = (1 + \overbrace{c(x) + c(z)}^{=0}) \text{ mod } 2 \\
    &= 1
    \end{align*}
    Es un ciclo impar, entonces no se puede colorear con 2 colores $\Rightarrow \chi(G)\geq 3. \blacksquare$
\end{demostracion}

\section{Enunciar y probar el Teorema de Hall.}

Sea $G=(V,E)$ bipartito con partes $X$ e $Y$. Existe un matching completo de $X$ a $Y$ si y solo si 
\begin{center}
    $|\Gamma(S)| \geq |S| \quad \forall S \subseteq X$
\end{center}
Notemos que un matching $M$ de $X$ a $Y$ es completo si y solo si $|M|=|X|$.
\textbf{Demostración.}\\
\textbf{($\Rightarrow$)}\\  
Sea $M$ un matching completo de $X$ a $Y$, este matching nos induce a una función inyectiva
\begin{center}
    $f: X \rightarrow Y$
\end{center}
tal que 
\begin{center}
    $f(x)\in \Gamma(x)$
\end{center}
Como $f$ es inyectiva tenemos que:
\begin{center}
    $|f(S)|=|S| \quad \forall S \subseteq X$
\end{center}
Finalmente 
\begin{center}
    $f(S) \subseteq \Gamma(S) \Rightarrow |S| \leq |\Gamma(S)| \quad \forall S \subseteq X$
\end{center}
\textbf{($\Leftarrow$)}\\ 
Supongamos que se cumple la condición de Hall, es decir: 
\begin{center}
    $|\Gamma(S)| \geq |S| \quad \forall S \subseteq X$
\end{center}
Y también asumamos que al correr el algoritmo para hallar un matching maximal, llegamos a un matching maximal $M$ tal que
$|E(M)| < |X|$. Es decir, un matching incompleto en $X$ (podría ser en $Y$). Construiremos un $S \subseteq X$ que viola la condición de Hall, es decir tal que
\begin{center}
    $|\Gamma(S)| < |S|$
\end{center}
llegando a un absurdo.\\
Corramos el algoritmo de extensión de matching sobre el ultimo matching $M$. Como esta matching no cubre a $X$, existen filas sin matchear, más otras filas etiquetadas.
Entonces sean:
\begin{align*}
    S & = \{\text{\small filas etiquetadas}\} \\
    T & = \{\text{\small columnas etiquetadas}\} \\
    S_0 & = \{\text{\small filas etiquetadas con }*\} \\
    T_1 & = \{\text{\small columnas etiquetadas por }S_0\} \\
    S_1 & = \{\text{\small filas etiquetadas por } T_1\}
 \end{align*}
Y en general:
\begin{align*}
    T_{i+1} & = \{\text{\small columnas etiquetadas por }S_i\} \\
    S_i & = \{\text{\small filas etiquetadas por } T_i\}.
    \end{align*}
Notemos que como $M$ no es completo, tenemos algunas filas sin matchear, es decir que
\[S_0 \ne \emptyset \tag{0}\]
Las sucesiones son finitas y $S=S_0 \cup S_1 \cup \ldots$ con uniones disjuntas, y $T=T_1 \cup T_2 \cup \ldots$ con uniones disjuntas.\\
Cuando revisamos una columna pueden pasar dos cosas:
\begin{itemize}
    \item La columna está libre, entonces la matcheamos y extendemos el matching. Esto no pasa porque $M$ es maximal.
    \item La columna está matcheada con una fila, entonces etiquetamos únicamente esa fila.
\end{itemize}
Por lo tanto, el algoritmo nunca se detiene cuando pasa de algún $T_i$ a $S_i$. Es decir que el algoritmo se detiene solamente al pasar de un $S_k$ a un $T_{k+1}=\emptyset$\\
Observemos que, como cada columna etiqueta a la fila con la cual está matcheada, cada columna etiquetada etiqueta a una sola fila, entonces tenemos que:
\[|T_i|=|S_i| \tag{1}\]
Porque el $T_i$ crea al $S_i$\\ 
Y tambíen\\ 
$S=S_0 \cup S_1 \cup \ldots S_k$ con uniones disjuntas, y $T=T_1 \cup T_2 \cup \ldots T_k$ con uniones disjuntas.\\ 
Entonces finalmente tenemos que:
\begin{align*}
    |S| & = |S_0| + |S_1| + \cdots + |S_k| \\
        & = |S_0| + |T_1| + \cdots + |T_k| \quad\text{por (1)} \\
        & = |S_0| + |T| > |T| \quad\text{por (0)} 
\end{align*}
Por lo tanto $|S| > |T|$. Ahora demostremos que $T=\Gamma(S)$.
\begin{itemize}
    \item \(T \subseteq \Gamma(S)\): sea \(y \in T\), \(y\) tuvo que ser etiquetado por una fila de S, y como cada fila etiqueta a sus columnas vecinas es claro que \({y \in \Gamma(S).}\)
    \item \(\Gamma(S) \subseteq T\): Supongamos que existe un \(y \in \Gamma(S)\) que no está en T. Existe un \({x \in S}\) que es vecino de \(y\). Pero cuando revisamos \(x\), habríamos visto que \(y\) era vecino suyo, y por lo tanto lo habríamos etiquetado. Absurdo pues habíamos supueto \({y \not\in T}\), luego, \({\Gamma(S) \subseteq T.}\)
\end{itemize}
    
Finalmente, construimos un \(S \subseteq X\) tal que no se cumple la condición de Hall. Lo que es un absurdo pues habiamos supuesto que era cierta. Entonces este absurdo viene de suponer que el matching maximal no es completo.$\blacksquare$

\section{Enunciar y probar el teorema del matrimonio de Koenig}

Todo grafo bipartito regular tiene matching perfecto.

\begin{demostracion}
Dado $W \subseteq V$, definimos
\[
E_W = \{wu : w \in W\}
\]
Sean $X, Y$ las partes de $G$. Sea $S \subseteq X$. Sea $l \in E_S$. Se sigue que
\[
\exists x \in S, y \in Y : l = xy = yx
\]
Es decir que $y \in \Gamma(x)$. Pero $x \in S$. Entonces $y \in \Gamma(S)$. Entonces $l \in E_{\Gamma(S)}$. $\therefore E_S \subseteq E_{\Gamma(S)}$. Por lo tanto
\[
|E_S| \leq |E_{\Gamma(S)}| \tag{1}
\]
Calculemos en general $|E_W|$ cuando $W \subseteq X$ o $W\subseteq Y$. Si $wu \in E_W$, entonces $v \notin W$, pues $W \subseteq X \Rightarrow v \in Y$, $W \subseteq Y \Rightarrow v \in X$.\\
De lo anterior se sigue que
\[
E_W = \bigcup_{w \in W} \{wv : v \in \Gamma(w)\} \tag{2}
\]
donde la unión es disjunta. Luego
\[
|E_W| = \sum_{w \in W} |\Gamma(w)| = \sum_{w \in W} d(w) \tag{3}
\]
Como $G$ es regular, $d(w) = \delta = \Delta$. Luego
\[
|E_W| = \Delta |W| \tag{4}
\]
Usando (1) y (4) tenemos que
\[
|S| \Delta \leq |\Gamma(S)| \Delta \Rightarrow |S| \leq |\Gamma(S)| \tag{5}
\]
Como esto vale para todo $S \subseteq X$, el teorema de Hall implica que existe un matching completo de $X$ a $Y$. Si demostramos que $|X| = |Y|$, se seguirá que ese matching será perfecto.\\ \\
La primera forma de probar esto es ver que como $X, Y$ son las partes de $G, E = E_X = E_Y$. Entonces $|E_X| = |E_Y|$, de lo cual se sigue que $\Delta |X| = |Y| \delta \Rightarrow |X| = |Y|$.\\ \\
Otra forma de verlo es observar que, dado que existe un matching completo de $X$ a $Y, |X| \leq |Y|$. Pero la elección de $X$ sobre $Y$ en la prueba fue arbitraria. Por lo tanto vale lo mismo para $Y$. Luego $|X| = |Y|$.\\ \\
En ambos casos, el matching es perfecto. $\blacksquare$
\end{demostracion}

\section{Probar que si $G$ es bipartito entonces $\chi'(G) = \Delta(G)$.}
\textbf{Se evalua en diciemre de 2024}

\section{Probar la complejidad $O(n^4)$ del algoritmo Hungaro y dar una idea de como se la puede reducir a $O(n^3)$}
\textbf{Se evalua en diciemre de 2024}

\section{Enunciar y probar el teorema de la cota de Hamming}

Para todo código $C \in \{0,1\}^n$ con $\delta(c)=\delta$ y $t=\left\lfloor\frac{\delta-1}{2}\right\rfloor$ entonces:
\[
|C| \leq \frac{2^n}{1 + n + \binom{n}{2} + \cdots + \binom{n}{t}}
\]
\begin{demostracion}
Sea
\[
A = \bigcup_{v \in C} D_t(v)
\]
Buscamos $|A|$.\\ \\
Como $C$ corrige $t$ errores tenemos que
\[
D_t(v) \cap D_t(w) = \emptyset, \forall v, w \in C \text{ tales que } v \neq w
\]
Es claro que $A$ es unión disjunta\\
Ahora definimos
\[
S_r(v) = \{w \in C : d_H(v, w) = r\}
\]
De esta forma es claro que:
\[
D_t(v) = \bigcup_{r=0}^t S_r(v) \quad \text{unión disjunta}
\]
Sea $w \in S_r(v)$, hay un subconjunto de los $n$ bits de las palabras que tiene $r$ elementos tal que $w$ difiere de $v$ en esos $r$ bits. Entonces tenemos:
\begin{itemize}
    \item Dado $w \in S_r(v)$, podemos obtener $r$ bits en los que $v$ y $w$ difieren.
    \item Dado un conjunto de $r$ bits, podemos obtener un $w$ tal que $d_H(v,w) = r$
\end{itemize}
Así, existe una biyección entre $S_r(v)$ y el conjunto de subconjuntos de $r$ bits. Entonces la cardinalidad de estos conjuntos es la misma. Luego
\[
|S_r(v)| = \binom{n}{r} \implies |D_t(v)| = \sum_{r=0}^t \binom{n}{r}
\]
Finalmente tenemos:
\begin{align*}
    |A| & = \sum_{v\in C} |D_t(v)| \\
        & = \sum_{v\in C} \sum_{r=0}^t \binom{n}{r} \\
        & = |C| \sum_{r=0}^t \binom{n}{r} \\
        & \leq 2^n \quad \text{pues } A \subseteq \{0,1\}^n \implies |C| \leq \frac{2^n}{\sum_{r=0}^t \binom{n}{r}} \blacksquare
\end{align*}
\end{demostracion}
\section{Sea $C$ un código lineal con matriz de chequeo $H$. Entonces $\delta(C)$ es el mínimo número de columnas LD de $H$}
\textbf{Demostración.}\\
Sea
\(m = \min \{r : \exists r \text{ columnas LD de } H\}\).
Probaremos \(m = \delta(C)\).\\
Para ello probaremos que $\delta(C) \leq m$ y $\delta(C) \geq m$.\\
Denotaremos la $j$-ésima columna de $H$ como $H^(j)$.\\ \\
Por definición de $m$ existe $j_1, j_2, \ldots, j_r$ tal que $H^{(j_1)}, H^{(j_2)}, \ldots, H^{(j_r)}$ son LD.\\ \\
Entonces existen $c_1, c_2, \ldots, c_r$ no todos 0 tales que:
\[
c_1H^{(j_1)} + c_2H^{(j_2)} + \cdots + c_rH^{(j_r)} = 0
\]
Sea $e_i = (0, 0, \ldots, 1, \ldots, 0)$ con 1 en la posición $i$. Entonces: 
\[
    He_i^t = \left[
        \begin{array}{c}
        \hspace{8mm} i \hspace{8mm} \\
        \vdots \\
        \hspace{8mm} i \hspace{8mm} \\
        \vdots \\
        \hspace{8mm} i \hspace{8mm}
        \end{array}
        \right]
        \begin{bmatrix}
        0 \\
        \vdots \\
        1 \\
        \vdots \\
        0
        \end{bmatrix} = H^{(i)}
\]
Es la columna $i-$ésima de $H$.
\newpage
Sea $x = c_1e_{j_1} + c_2e_{j_2} + \cdots + c_re_{j_r}$, como no todos los $c_j$ son 0 entonces $x \neq 0$, por lo que:
\begin{align*}
    H x^t &= H(c_1 e_{j_1}^t + c_2 e_{j_2}^t + \cdots + c_r e_{j_r}^t) \\
          &= c_1 H e_{j_1}^t + c_2 H e_{j_2}^t + \cdots + c_r H e_{j_r}^t \\
          &= c_1 H^{(j_1)} + c_2 H^{(j_2)} + \cdots + c_r H^{(j_r)} = 0 \\
          &\Rightarrow H x^t = 0
\end{align*}

$\Rightarrow x \in C$ pues $C = Nu(H)$, pero su peso es $\leq m$, pues es suma de a lo sumo $m$ vectores de peso 1.
y vimos que $x \neq 0$ entonces sabemos que:
\[
\delta(C) = \min \{ |v| : v \in C, v \neq 0 \}
\]

\[
\text{Por lo tanto como } x \in C \text{ y } x \neq 0, \text{ entonces } x \text{ está en ese conjunto, así que tenemos:}
\]

\[
\delta(C) = \min \{ |v| : v \in C, v \neq 0 \} \leq |x|
\]
Pero su peso es $\leq m$, entonces:
\[
\delta(C) = \min \{ |v| : v \in C, v \neq 0 \} \leq |x| \leq m
\]
Entonces, $\delta(C) \leq m$.\\ \\
Ahora probaremos que $\delta(C) \geq m$.\\
Sea $x \neq 0 : \delta(C) = |x|$ entonces:
\begin{align*}
    x &= c_1 e_{i_1} + \ldots + c_{\delta(C)} e_{i_{\delta(C)}} \\
      &= 0 \quad \{\text{como} \text{ } x \in C, \text{entonces} \text{ } x \neq 0\} \\
      &= H x^t = H^{(i_1)} + \ldots + H^{(i_{\delta(C)})} \\
      &\Rightarrow \{H^j, \ldots H^{i_{\delta(C)}}\} \text{son LD} \implies \delta(C) \geq m 
\end{align*}
$\blacksquare$

\section{Sea $C$ un código cíclico de dimensión $k$ y longitud $n$ y sea $g(x)$ su polinomio generador. Probar i, ii, iii y iv}
\begin{enumerate}[label=\roman*.]
    \item $C$ está formado por los multiplos de $g(x)$ de grado menor que $n$:
    \[
    C = \{p(x) : gr(p) < n\&g(x)|p(x)\}
    \]
    \item $C = \{v(x) \odot g(x): v \text{ } \text{es un polinomio cualquiera} \}$ 
    \item $gr(g(x)) = n - k$
    \item $g(x)$ divide a $1 + x^n$
\end{enumerate}
\textbf{Demostración}
\subsubsection*{Prueba de i. y ii.}

Sea $C_1 = \{p(x) : gr(p) < n\&g(x)|p(x)\}$ y $C_2 = \{v(x) \odot g(x): v \text{ } \text{es un polinomio cualquiera} \}$.\\ 
Vamos a demostrar que $C \subseteq C_1$, $C_2 \subseteq C$ y $C_1 \subseteq C_2$.\\ \\
$\boldsymbol{C_1 \subseteq C_2}$\\ \\
Sea $p(x) \in C_1$, queremos ver si está en $C_2$. Entonces existe $q(x)$ tal que $p(x) = g(x) \cdot q(x)$, además:
\[
n > gr(p) = gr(g(x) \cdot q(x)) 
\]
Luego tomando modulo tenemos que 
\begin{align*}
    p(x) \mod (1 + x^n) &= g(x) \cdot q(x) \mod (1 + x^n) \\
                        &= g(x) \odot q(x) \in C_2  
\end{align*} 
Por lo tanto $p(x) \in C_2$.\\ \\
$\boldsymbol{C_2 \subseteq C}$\\ \\
$\text{Sea } p(x) = v(x) \odot g(x) \in C_2, \text{ con } v(x) \text{ un polinomio cualquiera de la forma}$
\[
v(x) = v_0 + v_1 \cdot x + v_2 \cdot x^2 + \cdots + v_{gr(v)} \cdot x^{gr(v)}
\]
Queremos ver que $p(x) \in C$. Entonces:
\begin{align*}
    p(x) &= v(x) \odot g(x) \\
    &= (v_0 + v_1 \cdot x + v_2 \cdot x^2 + \cdots + v_{gr(v)} \cdot x^{gr(v)}) \odot g(x) \\
    &= v_0 \odot g(x) + v_1 \cdot (x \odot g(x)) + v_2 \cdot (x^2 \odot g(x)) + \cdots + v_{gr(v)} \cdot (x^{gr(v)} \odot g(x)) \\
    &= v_0 \cdot g(x) + v_1 \cdot Rot(g(x)) + v_2 \cdot Rot^2(g(x)) + \cdots + v_{gr(v)} \cdot Rot^{gr(v)}(g(x)) \\
    &\in C
\end{align*}  
Ya que todas las rotaciones de $g(x)$ están en $C$, entonces $p(x) \in C$.
\newpage
$\boldsymbol{C \subseteq C_1}$\\ \\
Sea $p(x) \in C$, como las palabras de $C$ tienen longitud $n$, entonces $gr(p) < n$. Nos queda ver que $g(x)|p(x)$. Para ello dividamos $p$ por $g$:
\[
\exists q(x), r(x) : p(x) = g(x) \cdot q(x) + r(x) \quad \text{ con } gr(r) < gr(g)
\]
Ahora tomemos modulo:
\begin{align*}
    p(x) &= p(x) \mod (1 + x^n) \\
         &= (g(x) \cdot q(x) + r(x)) \mod (1 + x^n) \\
         &= g(x) \odot q(x) + (r(x) \mod (1 + x^n)) \\
         &= g(x) \odot q(x) + r(x) \quad \text{pues } gr(r) < gr(g) < n
\end{align*}
Por lo tanto tenemos que:
\[
r(x) = p(x) + g(x) \odot q(x)
\]
Y como $p \in C$ y $g(x) \odot q(x) \in C_2$, entonces $r(x) \in C$.\\
Pero como $g$ es el generador, este es el único polinomio no nulo de menor grado en $C$, y como $g(r) < gr(g)$, entonces $r(x) = 0$.\\
Finalmente, como $g(x)|p(x)$, entonces $p(x) \in C_1$.
\subsubsection*{Prueba de iii.}
Sea \(p(x) \in C\), entonces existe \(q(x)\) tal que \({ p(x) = g(x) \cdot q(x) .}\)\\ 
Además \({n > gr(p) = gr(g) + gr(q) }\), entonces \({ gr(q) < n - gr(g) .}\)\\
Ahora, sea un \(q(x)\) tal que \(gr(q) < n - gr(g)\), tenemos que \({ g(x) \cdot q(x) \in C. }\)\\ \\
Es decir, hay una biyección entre \(C\) y el conjunto de polinomios de grado menor a \({n - gr(g).}\) Entonces:
\begin{align*}
	|C| & = |\text{conjunto de polinomios de grado menor a } n - gr(g)| \\
      \iff & \\
  2^k & = 2^{n - gr(g)} \\
      \iff & \\
    k & = n - gr(g) \\
      \iff & \\
    gr(g) & = n - k \\
\end{align*}

\subsubsection*{Prueba de iv.}
Dividimos \(1+x^n\) por \(g(x)\):
\[ 
\exists q(x), r(x) : 1 + x^n = g(x) \cdot q(x) + r(x) \quad \text{ con } gr(r) < gr(g)
\]
Ahora, si tomamos módulo:
\begin{align*}
	0 & = (1 + x^n) \text{ mod } (1 + x^n) \\
    & = g(x) \cdot q(x) + r(x) \text{ mod } (1 + x^n) \\
    & = g(x) \odot q(x) + (r(x) \text{ mod } (1 + x^n)) \\
    & = g(x) \odot q(x) + r(x) \quad\text{pues } gr(r) < gr(g) < n \\
    &\Rightarrow r(x) = g(x) \odot q(x) \in C
\end{align*}
Pero como $g$, es el polinomio de $C$ no nulo de menor grado y \({gr(r) < gr(g),}\) entonces \({r(x) = 0}\) y \({g(x) | (1 + x^n).}\). $\blacksquare$


\section{Probar que 3SAT es NP-completo}
Vamos a demostrar que 3-Color es NP-COMPLETO, para ello usaremos
\begin{itemize}
    \item SAT $\rightarrow$ 3-SAT $\rightarrow$ 3-Color
    \item Para el salto de 3-SAT a 3-Color usaremos un grafo bastante complejo que se puede colorear con 3 colores
    \item Cabe resaltar que el salto de 3 a 2 es NP-COMPLETO por esto.
\end{itemize}
Veamos que que SAT $\leq_p$ 3-SAT.\\ \\
Entonces, sea B en forma conjuntiva normal (CNF) tal que $B = D_1 \wedge \ldots \wedge D_r$ con
$D_j = l_{1,j} \vee l_{2,j} \vee \ldots \vee l_{i,j}$ con $l_{r_j,j}$ literales.\\ \\
Queremos construir $\tilde{B}$ polinomialmente tal que $\tilde{B}$ esté en CNF con 3 literales por
disyunción tal que B sea satisfacible si y solo si $\tilde{B}$ es satisfacible.\\ \\
Definimos unos $E_j$ y $\tilde{B} = E_1 \wedge \ldots \wedge E_n$.\\ \\
Cada $E_j$ lo definimos a partir de $D_j$, y procedemos de diferente manera según la
cantidad de literales $r_j$ presentes en $D_j$, entonces:
\begin{quote}
    Si $r_j = 3$ tenemos que $E_j = D_j$, por lo que no hacemos nada.\\
    Si $r_j < 3$ agregamos variables mudas que no afectan el resultado, vemos los casos:
        \begin{itemize}
            \item $r_j = 2 \rightarrow D_j = l_{1,j} \vee l_{2,j}$\\
                  $E_j = (l_{1,j} \vee l_{2,j} \vee y_j) \wedge (l_{1,j} \vee l_{2,j} \vee \bar{y}_j)$
            \item $r = 1 \rightarrow D_j = l_{1,j}$\\
                  $E_j = (l_{1,j} \vee y_{1,j} \vee y_{2.j}) \wedge (l_{1,j} \vee \bar{y}_{1,j} \vee y_{2,j}) \wedge (l_{1,j} \vee y_{1,j} \vee \bar{y}_{2,j}) \wedge (l_{1,j} \vee \bar{y}_{1,j} \vee \bar{y}_{2,j})$
            \end{itemize}
    Si $r_j > 3$ es el problema más grande:
    \[
        D_j = l_{1,j} \vee l_{2,j} \vee \ldots \vee l_{r_j,j}
    \]
    entonces:
    \begin{align*}
        E_j = &(l_{1,j} \vee l_{2,j} \vee y_{1,j}) \wedge (l_{3,j} \vee y_{2,j} \vee \bar{y}_{1,j}) \\
              &\wedge (l_{4,j} \vee y_{3,j} \vee \bar{y}_{2,j}) \wedge \ldots \wedge (l_{r_j-2,j} \vee y_{r_j-3,j} \vee \bar{y}_{r_j-4,j}) \\
              &\wedge (l_{r_j-1,j} \vee l_{r_j,j} \vee \bar{y}_{r_j-3,j})
    \end{align*}
    Son variables booleanas para evaluarlas tengo que elegir un vector de 0's y 1's.\\ \\
    \textbf{Ejemplo:} Supongamos $\tilde{B}$ satisfacible, $\tilde{B}$ es suma de función de variables $x_1,\ldots x_{algo}$ y
    variables extra $y_{i,j}$. Podríamos denotarlo con $\tilde{B}(\vec{x}, \vec{y})$ mientras que B depende solo de
    las $\vec{x}$, es decir, $B(\vec{x})$.\\
    Es decir:
    \[
        \underbrace{B = D_1 \wedge \ldots \wedge D_n}_{\vec{x}}
        \rightarrow 
        \underbrace{\tilde{B} = E_1 \wedge \ldots \wedge E_n}_{\vec{x},\vec{y}}
    \]
    Vamos a demostrar que si $\exists \vec{a} : B(\vec{a}) = 1 \Leftrightarrow \exists \vec{c}, \vec{b} : \tilde{B}(\vec{c}, \vec{b}) = 1$. (la idea es ver
    que $\vec{a} = \vec{c}$)\\ \\
    ($\Leftarrow$) Supongamos que $\tilde{B}(\vec{c}, \vec{b}) = 1$ queremos probar $B(\vec{a}) = 1$. Supongamos que
    no se cumple y llegaremos a un absurdo, entonces se da $B(\vec{a}) = 0$.\\ \\
    Como $B = D_1 \wedge \ldots \wedge D_n$ entonces $\exists j : D_j(\vec{a}) = 0$, pero $D_j = l_{1,j} \vee l_{2,j} \vee \ldots \vee l_{r_j,j}$
    donde todos los términos tienen que ser cero.\\ \\
    Entonces, $l_{i,j}(\vec{a}) = 0 \quad \forall i$ (y ese j). Por otro lado $\tilde{B}(\vec{a}, \vec{b}) = 1 \Rightarrow \exists E_j(\vec{a}, \vec{b}) = 1 \quad \forall j$
    en particular para ese j que mencionamos antes.\\
    Ahora tenemos que ver los casos:

    \begin{itemize}
        \item Si $r_j = 3$ tenemos que $E_j = D_j$ así que esto es imposible. $D_j(\vec{a}) = 0 \Rightarrow E_j = 0 \Rightarrow
        \tilde{B}(\vec{a}, \vec{b}) = 0$ Absurdo.
        
        \item Si $r_j = 2$ tenemos que $E_j = (l_{1,j} \vee l_{2,j} \vee y_j) \wedge (l_{1,j} \vee l_{2,j} \vee \bar{y}_j)$ pero $l_{i,j}(\vec{a}) = 0$ entonces:
        \begin{align*}
        1 &= E_j(\vec{a}, \vec{b}) \\
        &= (0 \vee 0 \vee y_j(\vec{b})) \wedge (0 \vee 0 \vee \bar{y}_j(\vec{b})) \\
        &= y_j(\vec{b}) \wedge \bar{y}_j(\vec{b}) \\
        1 &= 0
        \end{align*}
        Absurdo.
        
        \item $r_j = 1$
        \begin{align*}
        1 &= E_j(\vec{a}, \vec{b}) \\
        &= (l_{1,j} \vee y_{1,j} \vee y_{2,j}) \wedge (l_{1,j} \vee y_{1,j} \vee \bar{y}_{2,j}) \wedge (l_{1,j} \vee \bar{y}_{1,j} \vee y_{2,j}) \\
        &\wedge (l_{1,j} \vee \bar{y}_{1,j} \vee \bar{y}_{2,j})[\vec{a}, \vec{b}] \\
        &= (p \vee q) \wedge \underbrace {(\bar{p} \vee q) \wedge (p \vee \bar{q})}_{sii} \wedge(\bar{p} \vee \bar{q}) \\
        &= 0
        \end{align*}
        Absurdo.
        
        \item $r_j > 4$
        \begin{align*}
        1 &= E_j(\vec{a}, \vec{b}) \\
        &= (l_{1,j} \vee l_{2,j} \vee y_{1,j}) \wedge (l_{3,j} \vee \bar{y}_{1,j} \vee y_{2,j}) \wedge (l_{4,j} \vee \bar{y}_{2,j} \vee y_{3,j}) \wedge \ldots \\
        &\wedge (l_{r_j-2,j} \vee \bar{y}_{r_j-4,j} \vee y_{r_j-3,j}) \wedge (\bar{y}_{r_j-3,j} \vee l_{r_j-1,j} \vee l_{r_j,j})[\vec{a}, \vec{b}]
        \end{align*}
        sabemos que: $l_{i,j}(\vec{a}) = 0$\\
        si $p_i = y_{i,j}(\vec{b}) = p_1 \wedge (\bar{p}_1 \vee p_2) \wedge (\bar{p}_2 \vee p_3) \wedge \ldots \wedge (\bar{p}_{r_j-4} \vee p_{r_j-3}) \wedge \bar{p}_{r_j-3}$
        
        $p_1 \wedge (p_1 \Rightarrow p_2) \wedge (p_2 \Rightarrow p_3) \wedge \ldots \wedge (p_{r_j-4} \Rightarrow p_{r_j-3}) \wedge \bar{p}_{r_j-3} = 0$
        
        Todos tienen que ser 1 y el último 0, es un absurdo.
        \end{itemize}
    ($\Rightarrow$) Si $\exists \vec{a} : B(\vec{a}) = 1 \Rightarrow \exists \vec{b} : \tilde{B}(\vec{a}, \vec{b}) = 1$\\ \\
    Para $r_j \leq 3$ se le puede dar cualquier valor a los $y_{i,j}$ por ejemplo 0 y es trivial ver los
    casos $r_j = 1$ y $r_j = 2$. \\ \\
    El único problema grande es $r_j > 3$\\ \\
    Como $B(\vec{a}) = 1$ entonces $D_j(\vec{a}) = 1 \quad \forall j$ (y al menos un término de $D_j$ es 1). Entonces,
    $\exists i_j : l_{i_j,j}(\vec{a}) = 1$. Si hay más de uno tomo cualquiera, por ejemplo el primero.\\ \\
    Evaluamos los $y_{i,j}$ de forma tal que:
    \[
        y_{i,j}(\vec{b}) = 
        \begin{cases}
        1 & \text{si } i = 1, \ldots i_j - 2 \\
        0 & \text{si } i \geq i_j - 1
        \end{cases}
    \]
    Entonces, si evaluamos tenemos:
    \begin{align*}
        E_j(\vec{a}, \vec{b}) &= (l_{1,j} \vee l_{2,j} \vee \underbrace{y_{1,j}}_{=1}) \wedge
        (l_{3,j} \vee \bar{y}_{1,j} \vee \underbrace{y_{2,j}}_{=1}) \wedge \ldots \wedge
        (l_{i_j-1,j} \vee \bar{y}_{i_j-3,j} \vee \underbrace{y_{i_j-2,j}}_{=1}) \wedge \\
        &(\underbrace{l_{i_j,j}}_{=1(t)} \vee \bar{y}_{i_j-2,j} \vee y_{i_j-1,j}) \wedge
        (l_{i_j+1,j} \vee \underbrace{\bar{y}_{i_j-1,j}}_{=1} \vee y_{i_j,j}) \wedge \ldots \\
        &= 1
        \end{align*}
    $\blacksquare$

\end{quote}













\section{Probar que 3COLOR es NP-completo}
Para probar esto, sabiendo que 3SAT es NP completo, demostraremos que 3SAT se reduce polinomialmente a 3COLOR.\\
Es decir, debemos, dada una instancia de 3SAT, es decir, una expresion booleana $B$ en CNF con exactamente 3 literales
por disjuncion, crear polinomialmente una instancia de 3COLOR, es decir, un grafo $G$, tal que $B$ es satisfacible si y solo si $G$
se puede colorear con 3 colores.\\ \\
Sean $x_1 \ldots x_n$ las variables de $B$ y $D_j$ disjunciones cada una con tres literales tales que:
\[
B = D_1 \land D_2 \land \ldots \land D_m
\]
Como cada $D_j$ tiene 3 literales, sean $\ell_{jr}$, $r = 1, 2, 3$ los 3 literales de $D_j$. Es decir, $D_j = \ell_{j1} \vee \ell_{j2} \vee \ell_{j3}$.\\ \\
Construiremos ahora $G$.\\
Primero daremos los vertices de $G$:
\begin{itemize}
    \item Tendremos para cada $i = 1, 2, \ldots, n$ dos vértices $u_i$, $w_i$.
    \item Además, para cada $j = 1, \ldots, m$ tendremos 6 vértices $a_{j1}$, $a_{j2}$, $a_{j3}$, $e_{j1}$, $e_{j2}$, $e_{j3}$.
    \item Finalmente, dos vértices especiales, que llamaremos el CAPITAN y el AVISPON.
\end{itemize}
Observemos que la construcción de los vértices es directa a partir de solamente saber $n$ y $m$ y es polinomial.
\newpage
Ahora daremos los lados\\
Para poder definir los lados, debemos definir una función $v$ del conjunto de literales $\{x_1, .., x_n, \neg x_1, ..., \neg x_n\}$ en el conjunto
$\{u_1, w_1, ..., u_n, w_n\}$ de la siguiente forma:
\[
v(x_i) = u_i \quad v(\neg x_i) = w_i
\]
Es decir, $v$ de un literal es $u_i$ si el literal es la $i$-ésima variable, y $w_i$ si el literal es la $i$-ésima negación de una variable.\\
Entonces definimos los siguientes lados:
\begin{itemize}
    \item Triángulos $\{a_{j1}a_{j2}, a_{j2}a_{j3}, a_{j1}a_{j3}\}$, $j = 1, .., m$.
    \item Unir bases con extremos: $a_{jr}e_{jr}$, $j = 1, .., m$, $r = 1, 2, 3$. (Nota: los triángulos anteriores más estos últimos lados se
    llaman las “garras” porque al dibujarlos parecen unas garras).
    \item Unir extremos con el CAPITAN: $CAPITAN e_{jr}$, $j = 1, .., m$, $r = 1, 2, 3$.
    \item Triángulos basados en el avispon: $AVISPON u_i$, $AVISPON w_i$, $u_i w_i$, $i = 1, \ldots, n$.
    \item $(CAPITAN)(AVISPON)$ es otro lado.
    \item Lados $e_{jr}v(\ell_{jr})$, $j = 1, .., m$, $r = 1, 2, 3$.
\end{itemize}
Los primeros 4 tipos de lados dependen solo de $n$ y $m$, son los últimos lados los que capturan la “estructura” de $B$.\\
Está claro que la construcción es polinomial, porque solo leemos quiénes son $n$, $m$ y los literales $\ell_{jr}$ para construir $G$ y el
número de vértices y lados es lineal en estos números.\\ \\
Como $G$ tiene triángulos, sabemos que $\chi(G) \geq 3$. Demostremos entonces que $B$ es satisfacible si y solo si $\chi(G) = 3$.
\subsubsection*{$B$ es satisfacible $\Rightarrow \chi(G) = 3$}
Como $B$ es satisfacible, existe $\vec{b}$ un vector de bits tal que $B(\vec{b}) = 1$.\\
Colorearemos $G$ a partir de $\vec{b}$, y cada vez que coloreemos un vértice o conjunto de vértices debemos asegurarnos que el
coloreo sigue siendo propio.\\
Para empezar definimos:
\begin{itemize}
    \item Color del CAPITAN: ESCARLATA
    \item Color del AVISPON: VERDE
    \item Entonces el lado $(CAPITAN)(AVISPON)$ no crea problemas porque sus extremos tienen colores distintos.
\end{itemize}
Luego definimos:
\begin{itemize}
    \item Color de $u_i$ = ESCARLATA si $b_i = 1$ y NEGRO si $b_i = 0$.
    \item Color de $w_i$ = ESCARLATA si $b_i = 0$ y NEGRO si $b_i = 1$.
\end{itemize}
Entonces cada triángulo $AVISPON u_i$, $AVISPON w_i$, $u_i w_i$ tienen los colores VERDE, ESCARLATA y NEGRO en algún
orden, así que no crea problemas.\\ \\
Todavía no usamos que $B(\vec{b}) = 1$. Esa propiedad implica que $D_j(\vec{b}) = 1$ para todo $j$, y como $D_j$ es una disyunción eso
implica que:
\[
\forall j \in \{1, \ldots, m\} \exists r_j \in \{1, 2, 3\} : \ell_{jr_j}(\vec{b}) = 1
\]
(Si hay más de un “$r_j$“ elegimos uno solo)\\
Entonces coloreamos, para cada $j = 1, \ldots, m$:
\begin{itemize}
    \item Color de $a_{jr_j}$: VERDE
    \item Color de $a_{jk}$ para $k \neq r_j$: Uno NEGRO y el otro ESCARLATA.
\end{itemize}
De esta forma los triángulos $a_{j1}a_{j2}$, $a_{j2}a_{j3}$, $a_{j1}a_{j3}$ no crean problemas porque sus vértices tienen los tres colores distintos.\\
Luego coloreamos los extremos:
\begin{itemize}
    \item Color de $e_{jr_j}$: NEGRO
    \item Color de $e_{jk}$ para $k \neq r_j$: VERDE
\end{itemize}
Entonces:
\begin{itemize}
    \item Los lados $a_{jr_j}e_{jr_j}$ no crean problemas porque uno de sus extremos es VERDE y el otro NEGRO.
    \item Y para $k \neq r_j$, los lados $a_{jk}e_{jk}$ no crean problemas porque uno de sus extremos es NEGRO o ESCARLATA y el otro VERDE.
    \item Además los lados $CAPITAN e_{jk}$ ($k = 1, 2, 3$) no crean problemas porque el color del CAPITAN es ESCARLATA y el de los $e_{jk}$ es NEGRO o VERDE.
    \item Los lados $e_{jk}v(\ell_{jk})$ para $k \neq r_j$ no crean problemas porque el color de $e_{jk}$ es VERDE para esos $k$, mientras que el color
    de $v(\ell_{jk})$, que será un $u_i$ o $w_i$, va a ser NEGRO o ESCARLATA.
    \item Finalmente, quedan los lados $e_{jr_j}v(\ell_{jr_j})$. El color de $e_{jr_j}$ es NEGRO así que acá podría haber algún problema. Lo que
    debemos probar para evitar el problema es que el color de $v(\ell_{jr_j})$ es ESCARLATA.
\end{itemize}
Para probar esto, debemos analizar qué tipo de literal es $\ell_{jr_j}$:
\begin{enumerate}
    \item Si $\ell_{jr_j}$ es una variable:\\
    \begin{itemize}
        \item Entonces existe $i$ tal que $\ell_{jr_j} = x_i$.
        \item $\ell_{jr_j} = x_i$ implica por definición de $v$ que $v(\ell_{jr_j}) = u_i$ (*)
        \item Pero también, $\ell_{jr_j} = x_i$ implica que: $\ell_{jr_j}(\vec{b}) = x_i(\vec{b}) = b_i$.
        \item Como $1 = \ell_{jr_j}(\vec{b})$, concluimos que $b_i = 1$.
        \item Esto implica que el Color de $u_i$ es ESCARLATA. (**)
        \item Por lo tanto (*) y (**) nos dicen que, efectivamente, el color de $v(\ell_{jr_j})$ es ESCARLATA como queríamos.
    \end{itemize}
    \item Si $\ell_{jr_j}$ es la negación de una variable:
    \begin{itemize}
        \item Entonces existe $i$ tal que $\ell_{jr_j} = \neg x_i$.
        \item Por lo tanto $v(\ell_{jr_j}) = w_i$, así que queremos probar que el color de $w_i$ es ESCARLATA.
        \item  Tenemos: $1 = \ell_{jr_j}(\vec{b}) = (\neg x_i)(\vec{b}) = 1 - b_i$.
        \item  De lo cual concluimos que $b_i = 0$. Esto, por definición del coloreo, implica que el color de $w_i$ es ESCARLATA, como
        queríamos.
    \end{itemize}
\end{enumerate}
Fin $\Rightarrow$.
\subsubsection*{$B$ es satisfacible $\Leftarrow \chi(G) = 3$}
Sea $c$ un coloreo propio con 3 colores de $G$.\\
Definimos un vector de bits $\vec{b}$ como:\\
$b_i = 1$ si $c(u_i) = c(CAPITAN)$ y $b_i = 0$ si no.\\
Para demostrar que $B(\vec{b})$ es necesario y suficiente ver que $D_j(b) = 1$ para todo $j$.\\
Fijemos entonces un $j$ en $\{1, 2, .., m\}$
\begin{enumerate}
    \item  Como tenemos el triángulo $\{a_{j1}a_{j2}, a_{j2}a_{j3}, a_{j1}a_{j3}\}$, y $c$ es un coloreo propio con 3 colores, entonces los 3 colores deben
    aparecer en los vértices de ese triángulo.\\
    Por lo tanto, existe $r \in \{1, 2, 3\}$ tal que $c(a_{jr}) = c(AVISPON)$.
    \item Como $a_{jr}e_{jr}$ es un lado, $c(e_{jr}) \neq c(a_{jr})$, por lo tanto por [1], concluimos que $c(e_{jr}) \neq c(AVISPON)$.
    \item Como $CAPITAN e_{jr}$ es un lado, concluimos que $c(e_{jr}) \neq c(CAPITAN)$.
    \item Los items [2] y [3] nos dicen que el color de $e_{jr}$ debe ser igual al "tercer" color, es decir, el color que no es igual ni al
    color del AVISPON ni al color del CAPITAN. (Y, como $(CAPITAN)(AVISPON)$ es un lado, sabemos que el color del
    CAPITAN y el color del AVISPON son distintos, así que efectivamente, $e_{jr}$ debe tener el tercer color.
    \item Como $e_{jr}v(\ell_{jr})$ es un lado, entonces $c(v(\ell_{jr})) \neq c(e_{jr})$. Es decir, por el item [4], el color de $v(\ell_{jr})$ NO ES el tercer color.
    \item Como $AVISPON v(\ell_{jr})$ es un lado, entonces $c(v(\ell_{jr})) \neq c(AVISPON)$.
    \item Items [5] y [6] implican que $c(v(\ell_{jr})) = c(CAPITAN)$.
\end{enumerate}
A partir del item [7] veamos de probar que $D_j(\vec{b}) = 1$. \\
Para ello, otra vez, tenemos que analizar qué clase de literal es $\ell_{jr}$.
\begin{enumerate}[label=\roman*.]
    \item Si $\ell_{jr}$ es una variable:
    \begin{itemize}
        \item Entonces existe $i$ con $\ell_{jr} = x_i$. Por lo tanto $v(\ell_{jr}) = u_i$.
        \item Entonces [7] implica que $c(u_i) = c(CAPITAN)$, y esto, por definición de $\vec{b}$ implica que $b_i = 1$.
        \item Entonces $\ell_{jr}(\vec{b}) = x_i(\vec{b}) = b_i = 1$ lo cual implica $D_j(\vec{b}) = 1$ como queríamos.
    \end{itemize}
    \item Si $\ell_{jr}$ es la negación de una variable:
    \begin{itemize}
        \item Entonces existe $i$ con $\ell_{jr} = \neg x_i$. Por lo tanto $v(\ell_{jr}) = w_i$.
        \item Entonces [7] implica que $c(w_i) = c(CAPITAN)$.
        \item Como $u_i w_i$ es un lado, el color de $u_i$ debe ser distinto del color de $w_i$.
        \item Por lo tanto lo anterior implica que: $c(u_i) \neq c(CAPITAN)$.
        \item Esto, por definición de $\vec{b}$ implica que $b_i = 0$.
        \item Entonces: 
                        \[
                            \ell_{jr}(\vec{b}) = \neg x_i(\vec{b}) = 1 - b_i = 1 - 0 = 1
                        \]
            Lo cual otra vez implica que $D_j(\vec{b}) = 1$ como queríamos.
    \end{itemize}
\end{enumerate}
\(\blacksquare\)
\section{Probar que Matrimonio 3D (“matrimonio trisexual”) es NP completo}

\end{document}