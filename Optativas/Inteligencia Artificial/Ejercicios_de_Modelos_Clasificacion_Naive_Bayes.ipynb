{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ejercicios de Modelos Clasificacion Naive Bayes.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YTaQq9dCh0E"
      },
      "source": [
        "# **Ejercicios de Modelo de clasificación Naive Bayes**\n",
        "\n",
        "1.   Clasificación de palabras (por género de nombre)\n",
        "2.   Clasificación de correos (Ham o Spam)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rT7mmVnvFAcm"
      },
      "source": [
        "## Ejercicio de práctica: Clasificación de palabras por género\n",
        "\n",
        "**Objetivo:** Construye un classificador de nombres en español usando el siguiente dataset:\n",
        "https://github.com/jvalhondo/spanish-names-surnames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NphJqahkFnjO"
      },
      "source": [
        "1. **Preparación de los datos**: con un `git clone` puedes traer el dataset indicado a tu directorio en Colab, luego asegurate de darle el formato adecuado a los datos y sus features para que tenga la misma estructura del ejemplo anterior con el dataset `names` de nombres en ingles.\n",
        "\n",
        "* **Piensa y analiza**: ¿los features en ingles aplican de la misma manera para los nombres en español?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhbOT6_zFGHM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "9d439d73-22b9-4477-8b3a-d69e901d3cec"
      },
      "source": [
        "!git clone https://github.com/jvalhondo/spanish-names-surnames"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'spanish-names-surnames'...\n",
            "remote: Enumerating objects: 36, done.\u001b[K\n",
            "remote: Total 36 (delta 0), reused 0 (delta 0), pack-reused 36\n",
            "Unpacking objects: 100% (36/36), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtPvSNI6v_7i"
      },
      "source": [
        "# Cargamos los archivos csv con numpy para tenerlos en listas\n",
        "import numpy as np\n",
        "import nltk, random\n",
        "tag_men = np.genfromtxt('/content/spanish-names-surnames/male_names.csv', skip_header=1, delimiter=',', dtype=('U20','i8','f8'))\n",
        "tag_women = np.genfromtxt('/content/spanish-names-surnames/female_names.csv', skip_header=1, delimiter=',', dtype=('U20','i8','f8'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uuhrbn_UfLkU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "3524560b-c1c3-4fab-df83-27d20f74ea05"
      },
      "source": [
        "# Verificamos\n",
        "print(tag_men[0])\n",
        "print(tag_women[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('ANTONIO', 715215, 54.6)\n",
            "('MARIA CARMEN', 668639, 54.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71MiVI6wPOQR"
      },
      "source": [
        "# Juntamos las listas de hombres y mujeres\n",
        "f_set = [(name[0],'male') for name in tag_men] + [(name[0],'female') for name in tag_women]\n",
        "\n",
        "# Hacemos un shuffle\n",
        "random.shuffle(f_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuY1Ux30F9uZ"
      },
      "source": [
        "2. **Entrenamiento y performance del modelo**: usando el classificador de Naive Bayes de NLTK entrena un modelo sencillo usando el mismo feature de la última letra del nombre, prueba algunas predicciones y calcula el performance del modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdsOBzQcNQBr"
      },
      "source": [
        "# Entrenamos teniendo en cuenta nuestro primer atributo (Revisando la ultima letra del nombre)\n",
        "def atributo_uno(palabra):\n",
        "\treturn {'ultima_letra': palabra[-1].lower()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvHwHTS8GT9I"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "f_un_atributo = [(atributo_uno(n), g) for (n, g) in f_set]\n",
        "\n",
        "#Usamos el 80% de los datos para train y 20% para test\n",
        "f_un_atributo_train, f_un_atributo_test = train_test_split(f_un_atributo, test_size=0.20, random_state=45)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcALDMVPI5-W"
      },
      "source": [
        "# Entrenamos\n",
        "classifier_1 = nltk.NaiveBayesClassifier.train(f_un_atributo_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stzlUrt6L23y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3635eb0f-5aad-4d4d-aa5a-948231543aac"
      },
      "source": [
        "#Probamos\n",
        "classifier_1.classify(atributo_uno('daniela'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'female'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNX5JzBcJn84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "84427134-3712-4100-d202-f711faf5210a"
      },
      "source": [
        "# Sacamos el accuracy\n",
        "print(nltk.classify.accuracy(classifier_1, f_un_atributo_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7919537900283745\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4a2jv85GXA_"
      },
      "source": [
        "3. **Mejores atributos:** Define una función como `atributos2()` donde puedas extraer mejores atributos con los cuales entrenar una mejor version del clasificador. Haz un segundo entrenamiento y verifica como mejora el performance de tu modelo. ¿Se te ocurren mejores maneras de definir atributos para esta tarea particular?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9G-E5_CXIQiO"
      },
      "source": [
        "# Agregamos mas atributos para ver cuanto mejora el accuracy\n",
        "def atributos2(nombre):\n",
        "    atrib = {}\n",
        "    atrib[\"ultima_letra\"] = nombre[-1].lower() #Ultima letra\n",
        "    atrib[\"ultimas_dos_letra\"] = nombre[-1:-5:-1].lower() #ultimas 4 letras\n",
        "    return atrib\n",
        "\n",
        "f_varios_atributo = [(atributos2(n), g) for (n, g) in f_set]\n",
        "\n",
        "#Usamos el 80% de los datos para train y 20% para test\n",
        "f_varios_atributo_train, f_varios_atributo_test = train_test_split(f_varios_atributo, test_size=0.20, random_state=45)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVRySPBKoIya"
      },
      "source": [
        "# Entrenamos\n",
        "classifier_2 = nltk.NaiveBayesClassifier.train(f_varios_atributo_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9MX0Fn4oSl5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "09db0525-43d6-4c2e-8c57-f9b1e3494c7e"
      },
      "source": [
        "# Sacamos accuracy\n",
        "print(classifier_2.classify(atributos('Juan')))\n",
        "print(nltk.classify.accuracy(classifier_2, f_varios_atributo_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "male\n",
            "0.9079854073773814\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7CXFyfoGf4s"
      },
      "source": [
        "# Clasificación de documentos (email spam o no spam)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeBvifrnr3GY"
      },
      "source": [
        "### Ejercicio de práctica\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AR53vedlvd1O"
      },
      "source": [
        "¿Como podrías construir un mejor clasificador de documentos?\n",
        "\n",
        "0. **Dataset más grande:** El conjunto de datos que usamos fue muy pequeño, considera usar los archivos corpus que estan ubicados en la ruta: `datasets/email/plaintext/`\n",
        "\n",
        "1. **Limpieza:** como te diste cuenta no hicimos ningun tipo de limpieza de texto en los correos electrónicos. Considera usar expresiones regulares, filtros por categorias gramaticales, etc ... .\n",
        "\n",
        "---\n",
        "\n",
        "Con base en eso construye un dataset más grande y con un tokenizado más pulido."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlD3HesETGQi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "ebca6e30-1099-4628-ebad-eed3c544a2aa"
      },
      "source": [
        "!git clone https://github.com/pachocamacho1990/datasets\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk import word_tokenize"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'datasets' already exists and is not an empty directory.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "067OLDPoBSAr"
      },
      "source": [
        "# Descomprimir ZIP\n",
        "import zipfile\n",
        "fantasy_zip = zipfile.ZipFile('/content/datasets/email/plaintext/corpus1.zip')\n",
        "fantasy_zip.extractall('/content/datasets/email/plaintext')\n",
        "fantasy_zip.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QATPsXgq9Onv"
      },
      "source": [
        "# Creamos un listado de los archivos dentro del Corpus1 ham/spam\n",
        "from os import listdir\n",
        "\n",
        "path_ham = \"/content/datasets/email/plaintext/corpus1/ham/\"\n",
        "filepaths_ham = [path_ham+f for f in listdir(path_ham) if f.endswith('.txt')]\n",
        "\n",
        "path_spam = \"/content/datasets/email/plaintext/corpus1/spam/\"\n",
        "filepaths_spam = [path_spam+f for f in listdir(path_spam) if f.endswith('.txt')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eel9M6anSmJX"
      },
      "source": [
        "# Creamos la funcion para tokenizar y leer los archivos\n",
        "\n",
        "def abrir(texto):\n",
        "  with open(texto, 'r', errors='ignore') as f2:\n",
        "    data = f2.read()\n",
        "    data = word_tokenize(data)\n",
        "  return data\n",
        "\n",
        "# Creamos la lista tokenizada del ham\n",
        "list_ham = list(map(abrir, filepaths_ham))\n",
        "# Creamos la lista tokenizada del spam\n",
        "list_spam = list(map(abrir, filepaths_spam))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uY9A261UaDHB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "7270927a-b283-4e2a-d6eb-cf774d6811e5"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "\n",
        "# Separamos las palabras mas comunes\n",
        "all_words = nltk.FreqDist([w for tokenlist in list_ham+list_spam for w in tokenlist])\n",
        "top_words = all_words.most_common(250)\n",
        "\n",
        "# Agregamos Bigramas\n",
        "bigram_text = nltk.Text([w for token in list_ham+list_spam for w in token])\n",
        "bigrams = list(nltk.bigrams(bigram_text))\n",
        "top_bigrams = (nltk.FreqDist(bigrams)).most_common(250)\n",
        "\n",
        "\n",
        "def document_features(document):\n",
        "    document_words = set(document)\n",
        "    bigram = set(list(nltk.bigrams(nltk.Text([token for token in document]))))\n",
        "    features = {}\n",
        "    for word, j in top_words:\n",
        "        features['contains({})'.format(word)] = (word in document_words)\n",
        "\n",
        "    for bigrams, i in top_bigrams:\n",
        "        features['contains_bigram({})'.format(bigrams)] = (bigrams in bigram)\n",
        "\n",
        "    return features"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7NUoSzgfu7o"
      },
      "source": [
        "# Juntamos las listas indicando si tienen palabras de las mas comunes\n",
        "import random\n",
        "fset_ham = [(document_features(texto), 0) for texto in list_ham]\n",
        "fset_spam = [(document_features(texto), 1) for texto in list_spam]\n",
        "fset = fset_spam + fset_ham[:1500]\n",
        "random.shuffle(fset)\n",
        "\n",
        "# Separamos en las listas en train y test\n",
        "from sklearn.model_selection import train_test_split\n",
        "fset_train, fset_test = train_test_split(fset, test_size=0.20, random_state=45)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9V_KmDBHwiy8"
      },
      "source": [
        "2. **Validación del modelo anterior:**  \n",
        "---\n",
        "\n",
        "una vez tengas el nuevo conjunto de datos más pulido y de mayor tamaño, considera el mismo entrenamiento con el mismo tipo de atributos del ejemplo anterior, ¿mejora el accuracy del modelo resultante?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AM6Vhy-Fw8oj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8aba116b-9599-46d6-a705-ca0bfa5a6dbd"
      },
      "source": [
        "# Entrenamos el programa\n",
        "classifier = nltk.NaiveBayesClassifier.train(fset_train)\n",
        "\n",
        "# Probamos y calificamos\n",
        "classifier.classify(document_features(list_ham[34]))\n",
        "print(nltk.classify.accuracy(classifier, fset_test))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8966666666666666\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}